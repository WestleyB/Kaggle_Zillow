{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Zillow Algo Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dropout, BatchNormalization, Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import gc\n",
    "from zillow_functions import create_newFeatures, data_preprocessing, memory_reduce\n",
    "from sami_function import missing_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train, prop and sample data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tShape train : (90275, 60)\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "\tOutliers treated ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 43.99 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 76 columns reduced\n",
      "\tFinal size 14.64 MB\n",
      "\n",
      "Building train set ...\n",
      "\tShape train : (90275, 75) Labels : (90275,)\n",
      "\tlen_x is : 75\n",
      "\n",
      "Building Neural Network ...\n",
      "\n",
      "Training Neural Network ...\n",
      "Epoch 1/60\n",
      "32s - loss: 0.0625\n",
      "Epoch 2/60\n",
      "31s - loss: 0.0605\n",
      "Epoch 3/60\n",
      "30s - loss: 0.0601\n",
      "Epoch 4/60\n",
      "31s - loss: 0.0600\n",
      "Epoch 5/60\n",
      "31s - loss: 0.0599\n",
      "Epoch 6/60\n",
      "31s - loss: 0.0599\n",
      "Epoch 7/60\n",
      "31s - loss: 0.0598\n",
      "Epoch 8/60\n",
      "31s - loss: 0.0598\n",
      "Epoch 9/60\n",
      "31s - loss: 0.0597\n",
      "Epoch 10/60\n",
      "30s - loss: 0.0597\n",
      "Epoch 11/60\n",
      "31s - loss: 0.0597\n",
      "Epoch 12/60\n",
      "30s - loss: 0.0597\n",
      "Epoch 13/60\n",
      "31s - loss: 0.0596\n",
      "Epoch 14/60\n",
      "30s - loss: 0.0596\n",
      "Epoch 15/60\n",
      "30s - loss: 0.0596\n",
      "Epoch 16/60\n",
      "30s - loss: 0.0595\n",
      "Epoch 17/60\n",
      "31s - loss: 0.0595\n",
      "Epoch 18/60\n",
      "30s - loss: 0.0595\n",
      "Epoch 19/60\n",
      "30s - loss: 0.0594\n",
      "Epoch 20/60\n",
      "30s - loss: 0.0594\n",
      "Epoch 21/60\n",
      "31s - loss: 0.0594\n",
      "Epoch 22/60\n",
      "30s - loss: 0.0593\n",
      "Epoch 23/60\n",
      "30s - loss: 0.0593\n",
      "Epoch 24/60\n",
      "31s - loss: 0.0593\n",
      "Epoch 25/60\n",
      "31s - loss: 0.0593\n",
      "Epoch 26/60\n",
      "30s - loss: 0.0592\n",
      "Epoch 27/60\n",
      "30s - loss: 0.0592\n",
      "Epoch 28/60\n",
      "31s - loss: 0.0592\n",
      "Epoch 29/60\n",
      "30s - loss: 0.0591\n",
      "Epoch 30/60\n",
      "30s - loss: 0.0591\n",
      "Epoch 31/60\n",
      "30s - loss: 0.0590\n",
      "Epoch 32/60\n",
      "31s - loss: 0.0591\n",
      "Epoch 33/60\n",
      "30s - loss: 0.0590\n",
      "Epoch 34/60\n",
      "30s - loss: 0.0590\n",
      "Epoch 35/60\n",
      "31s - loss: 0.0590\n",
      "Epoch 36/60\n",
      "31s - loss: 0.0589\n",
      "Epoch 37/60\n",
      "31s - loss: 0.0589\n",
      "Epoch 38/60\n",
      "30s - loss: 0.0589\n",
      "Epoch 39/60\n",
      "7226s - loss: 0.0589\n",
      "Epoch 40/60\n",
      "33s - loss: 0.0589\n",
      "Epoch 41/60\n",
      "2327s - loss: 0.0588\n",
      "Epoch 42/60\n",
      "37s - loss: 0.0587\n",
      "Epoch 43/60\n",
      "34s - loss: 0.0588\n",
      "Epoch 44/60\n",
      "35s - loss: 0.0587\n",
      "Epoch 45/60\n",
      "35s - loss: 0.0587\n",
      "Epoch 46/60\n",
      "36s - loss: 0.0587\n",
      "Epoch 47/60\n",
      "33s - loss: 0.0587\n",
      "Epoch 48/60\n",
      "32s - loss: 0.0586\n",
      "Epoch 49/60\n",
      "32s - loss: 0.0586\n",
      "Epoch 50/60\n",
      "34s - loss: 0.0585\n",
      "Epoch 51/60\n",
      "32s - loss: 0.0585\n",
      "Epoch 52/60\n",
      "33s - loss: 0.0585\n",
      "Epoch 53/60\n",
      "33s - loss: 0.0585\n",
      "Epoch 54/60\n",
      "33s - loss: 0.0585\n",
      "Epoch 55/60\n",
      "32s - loss: 0.0584\n",
      "Epoch 56/60\n",
      "32s - loss: 0.0584\n",
      "Epoch 57/60\n",
      "32s - loss: 0.0584\n",
      "Epoch 58/60\n",
      "32s - loss: 0.0584\n",
      "Epoch 59/60\n",
      "33s - loss: 0.0584\n",
      "Epoch 60/60\n",
      "33s - loss: 0.0584\n",
      "\n",
      "Building test set ...\n",
      "\n",
      "Working batch 100000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 200000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 300000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 400000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 500000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 600000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 700000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 800000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 900000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 1000000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 1100000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 1200000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 1300000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 1400000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 1500000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 1600000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 1700000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 1800000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 1900000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 2000000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 2100000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 2200000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 2300000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 2400000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 106.43 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 2500000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 2600000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 2700000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 2800000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 104.14 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 2900000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 327.30 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 105.29 MB\n",
      "\tShape test batch : (600000, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Working batch 3000000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 278.92 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 82 columns reduced\n",
      "\tFinal size 88.75 MB\n",
      "\tShape test batch : (511302, 75)\n",
      "\n",
      "Predicting on batch test ...\n",
      "\n",
      "Preparing results for write ...\n",
      "\n",
      "Writing results ...\n",
      "\n",
      "Prediction available !!!\n",
      "CPU times: user 1h 36min 55s, sys: 34min 46s, total: 2h 11min 42s\n",
      "Wall time: 4h 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Loading train, prop and sample data')\n",
    "train = pd.read_csv(\"../data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "prop = pd.read_csv('../data/properties_2016.csv')\n",
    "sample = pd.read_csv('../data/sample_submission.csv')\n",
    " \n",
    "#df_train =df_train[ df_train.logerror > -0.4005 ]\n",
    "#df_train=df_train[ df_train.logerror < 0.412 ]\n",
    "\n",
    "df_train = pd.merge(train, prop, on='parcelid', how='left')\n",
    "print('\\tShape train : {}'.format(df_train.shape))\n",
    "\n",
    "del train; gc.collect()\n",
    "\n",
    "print('\\nData preprocessing ...')\n",
    "df_train = data_preprocessing(df_train)\n",
    "\n",
    "\n",
    "print('\\nCreating new features ...')\n",
    "df_train = create_newFeatures(df_train)\n",
    "# New special feature\n",
    "# df_train['spe_feature'], nawFeature_mod = creature_special_feature(df_train[['transaction_year', 'transaction_month', 'yearbuilt', 'house_age']], df_train['logerror'].values)\n",
    "\n",
    "print('\\nReducing consumption memory ...')\n",
    "df_train = memory_reduce(df_train)\n",
    "\n",
    "\n",
    "print('\\nBuilding train set ...')\n",
    "x_train = df_train.drop(['parcelid', 'logerror'], axis=1)  \n",
    "y_train = df_train[\"logerror\"]\n",
    "y_mean = np.mean(y_train)\n",
    "print('\\tShape train : {} Labels : {}'.format(x_train.shape, y_train.shape))\n",
    "train_columns = x_train.columns\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "\n",
    "len_x = int(x_train.shape[1])\n",
    "print('\\tlen_x is : {}'.format(len_x))\n",
    "\n",
    "print('\\nBuilding Neural Network ...')\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units = 360 , kernel_initializer = 'normal', activation = 'tanh', input_dim = len_x))\n",
    "nn.add(Dropout(.17))\n",
    "nn.add(Dense(units = 150 , kernel_initializer = 'normal', activation = 'relu'))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.4))\n",
    "nn.add(Dense(units = 60 , kernel_initializer = 'normal', activation = 'relu'))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.32))\n",
    "nn.add(Dense(units = 25, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.22))\n",
    "nn.add(Dense(1, kernel_initializer='normal'))\n",
    "nn.compile(loss='mae', optimizer='adam')\n",
    "#classifier.compile(loss='mean_absolute_error', optimizer='rmsprop', metrics=['mae', 'accuracy'])\n",
    "\n",
    "\n",
    "print('\\nTraining Neural Network ...')\n",
    "nn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 60, verbose=2)\n",
    "nn.save('keras_model.h5')\n",
    "\n",
    "\n",
    "print('\\nBuilding test set ...')\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "\n",
    "del prop, sample; gc.collect()\n",
    "\n",
    "\n",
    "p_test = []\n",
    "batch_size = 100000\n",
    "for batch in range(batch_size, df_test.shape[0]+batch_size, batch_size):\n",
    "    \n",
    "    print('\\nWorking batch {}'.format(batch))\n",
    "    df_test_batch = df_test[batch-batch_size:batch].copy()\n",
    "    \n",
    "    print('\\nData preprocessing ...')\n",
    "    df_test_batch['rawcensustractandblock'] = df_test_batch.rawcensustractandblock.fillna(df_test.rawcensustractandblock.mode()[0])\n",
    "    df_test_batch = data_preprocessing(df_test_batch)\n",
    "    df_test_batch = df_test_batch.fillna(-1)\n",
    "    \n",
    "    print('\\nCreating new features ...')\n",
    "    \n",
    "    df_test_batch = create_newFeatures(df_test_batch)\n",
    "    # df_test_batch['spe_feature'], nawFeature_mod = creature_special_feature(df_test_batch[['transaction_year', 'transaction_month', 'yearbuilt', 'house_age']], model=nawFeature_mod)\n",
    "    \n",
    "    print('\\nReducing consumption memory ...')\n",
    "    \n",
    "    df_test_batch = memory_reduce(df_test_batch)\n",
    "\n",
    "    x_test_batch = df_test_batch[train_columns]\n",
    "    x_test_batch = sc.transform(x_test_batch)\n",
    "    \n",
    "    del df_test_batch; gc.collect()\n",
    "\n",
    "    print('\\tShape test batch : {}'.format(x_test_batch.shape))\n",
    "\n",
    "    print('\\nPredicting on batch test ...')\n",
    "    \n",
    "    y_pred_ann = nn.predict(x_test_batch)\n",
    "    y_pred_batch = y_pred_ann.flatten()\n",
    "    \n",
    "    del x_test_batch, y_pred_ann; gc.collect()\n",
    "    \n",
    "    [p_test.append(p) for p in y_pred_batch]\n",
    "    \n",
    "    \n",
    "print( \"\\nPreparing results for write ...\" )\n",
    "\n",
    "i = 0\n",
    "sub = pd.read_csv('../data/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = p_test[i::6]\n",
    "    i = i + 1\n",
    "\n",
    "print('\\nWriting results ...')\n",
    "sub.to_csv('../submissions/keras_nn_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False, float_format='%.4f')\n",
    "\n",
    "print('\\nPrediction available !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
