{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Zillow Algo Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import calendar\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection, preprocessing\n",
    "from scipy.stats import kendalltau\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "## Keras comes here\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from zillow_functions import create_newFeatures, data_preprocessing, memory_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train, prop and sample data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating new features ...\n",
      "Shape train: (90275, 60)\n",
      "\n",
      "Data preprocessing ...\n",
      "Create x_train and y_train from df_train\n",
      "(90275, 75) (90275,)\n",
      "Creating df_test  :\n",
      "Merge Sample with property data :\n",
      "\n",
      "Working batch 100000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 200000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 300000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 400000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 500000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 600000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 700000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 800000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 900000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 1000000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 1100000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 1200000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 1300000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 1400000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 1500000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 1600000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 1700000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 1800000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 1900000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 2000000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 2100000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 2200000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 2300000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 2400000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 2500000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 2600000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 2700000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Working batch 2800000\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Data preprocessing ...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9bccaff01ed4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;31m#     df_test_batch = memory_reduce(df_test_batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[0mdf_test_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_test_processed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mdf_test_batch\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    204\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                        copy=copy)\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;31m# consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m             \u001b[0mndims\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(self, inplace)\u001b[0m\n\u001b[0;32m   3154\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inplace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3156\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3157\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3158\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3136\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3138\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   3125\u001b[0m         \"\"\"\n\u001b[0;32m   3126\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3127\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3128\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3129\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m   3134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3135\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3136\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3138\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconsolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3571\u001b[0m         \u001b[0mbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3572\u001b[0m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3573\u001b[1;33m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3574\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3577\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3578\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3579\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3580\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   4523\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4524\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[1;32m-> 4525\u001b[1;33m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[0;32m   4526\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4527\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WBirmingham\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[0;32m   4546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4547\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4548\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4549\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load train, Prop and sample\n",
    "print('Loading train, prop and sample data')\n",
    "train = pd.read_csv(\"../data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "prop = pd.read_csv('../data/properties_2016.csv')\n",
    "sample = pd.read_csv('../data/sample_submission.csv')\n",
    " \n",
    "# print('Fitting Label Encoder on properties')\n",
    "# for c in prop.columns:\n",
    "#     prop[c]=prop[c].fillna(-1)\n",
    "#     if prop[c].dtype == 'object':\n",
    "#         lbl = LabelEncoder()\n",
    "#         lbl.fit(list(prop[c].values))\n",
    "#         prop[c] = lbl.transform(list(prop[c].values))\n",
    "        \n",
    "# #Create df_train and x_train y_train from that\n",
    "# print('Creating training set:')\n",
    "# df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "###########################################################\n",
    "# df_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\n",
    "# df_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\n",
    "# df_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\n",
    "# df_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\n",
    "# df_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "# print('Fill  NA/NaN values using suitable method' )\n",
    "# #df_train.fillna(df_train.mean(),inplace = True)\n",
    "# df_train.fillna(-1.0)\n",
    "\n",
    "#df_train =df_train[ df_train.logerror > -0.4005 ]\n",
    "#df_train=df_train[ df_train.logerror < 0.412 ]\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "#    My Preprocessing\n",
    "\n",
    "print('\\nCreating new features ...')\n",
    "\n",
    "df_train = pd.merge(train, prop, on='parcelid', how='left')\n",
    "print('Shape train: {}'.format(df_train.shape))\n",
    "del train; gc.collect()\n",
    "\n",
    "df_train = create_newFeatures(df_train)\n",
    "\n",
    "\n",
    "print('\\nData preprocessing ...')\n",
    "\n",
    "df_train = data_preprocessing(df_train)\n",
    "\n",
    "\n",
    "# print('\\nReducing consumption memory ...')\n",
    "\n",
    "# df_train = memory_reduce(df_train)\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print('Create x_train and y_train from df_train' )\n",
    "x_train = df_train.drop(['parcelid', 'logerror'], axis=1)  \n",
    "#  'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'\n",
    "y_train = df_train[\"logerror\"]\n",
    "\n",
    "#print(\"Bind x_train to float32:\")\n",
    "#x_train = x_train.values.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "print(x_train.shape, y_train.shape)\n",
    "train_columns = x_train.columns\n",
    "\n",
    "# for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "#     x_train[c] = (x_train[c] == True)\n",
    "# Create df_test and test set\n",
    "print('Creating df_test  :')\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "\n",
    "print(\"Merge Sample with property data :\")\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "\n",
    "\n",
    "########################\n",
    "# df_test[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\n",
    "# df_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\n",
    "# df_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\n",
    "# df_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\n",
    "# df_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day     \n",
    "\n",
    "#################################\n",
    "\n",
    "\n",
    "# x_test = df_test[train_columns]\n",
    "\n",
    "# print('Shape of x_test:', x_test.shape)\n",
    "# print(\"Preparing x_test:\")\n",
    "# for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "#     x_test[c] = (x_test[c] == True)\n",
    "\n",
    "# print(\"Bind x_test to float32:\")\n",
    "#x_test = x_test.values.astype(np.float32, copy=False)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "#   My preprocessing\n",
    "\n",
    "del prop, sample; gc.collect()\n",
    "\n",
    "df_test_processed = pd.DataFrame()\n",
    "p_test = []\n",
    "batch_size = 100000\n",
    "for batch in range(batch_size, df_test.shape[0]+batch_size, batch_size):\n",
    "    \n",
    "    print('\\nWorking batch {}'.format(batch))\n",
    "    \n",
    "    df_test_batch = df_test[batch-batch_size:batch].copy()\n",
    "    \n",
    "    print('\\nCreating new features ...')\n",
    "    \n",
    "    df_test_batch['rawcensustractandblock'] = df_test_batch.rawcensustractandblock.fillna(df_test.rawcensustractandblock.mode()[0])\n",
    "    df_test_batch = df_test_batch.fillna(0)\n",
    "    \n",
    "    df_test_batch = create_newFeatures(df_test_batch)\n",
    "    \n",
    "    print('\\nData preprocessing ...')\n",
    "\n",
    "    df_test_batch = data_preprocessing(df_test_batch)\n",
    "\n",
    "\n",
    "#     print('\\nReducing consumption memory ...')\n",
    "    \n",
    "#     df_test_batch = memory_reduce(df_test_batch)\n",
    "\n",
    "    df_test_processed = pd.concat([df_test_processed, df_test_batch[train_columns]])\n",
    "\n",
    "    del df_test_batch; gc.collect()\n",
    "\n",
    "df_test = df_test_processed.copy()\n",
    "                                   \n",
    "del df_test_processed; gc.collect()\n",
    "\n",
    "Print('Fin du preprocessing custom ...')\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "x_test = df_test[train_columns]\n",
    "\n",
    "print('Shape of x_test:', x_test.shape)\n",
    "\n",
    "###################\n",
    "##  PREPROCESS  ##\n",
    "###################\n",
    "\n",
    "\n",
    "#############################Imputer##################\n",
    "\n",
    "# from sklearn.preprocessing import Imputer\n",
    "# imputer= Imputer()\n",
    "# imputer.fit(x_train.iloc[:, :])\n",
    "# x_train = imputer.transform(x_train.iloc[:, :])\n",
    "# imputer.fit(x_test.iloc[:, :])\n",
    "# x_test = imputer.transform(x_test.iloc[:, :])\n",
    "\n",
    "#########################Standard Scalar##############\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "##  RUN NETWORK  ##\n",
    "###################\n",
    "\n",
    "\n",
    "len_x=int(x_train.shape[1])\n",
    "print(\"len_x is:\",len_x)\n",
    "#########################################################################\n",
    "####################ANN Starts here#\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units = 360 , kernel_initializer = 'normal', activation = 'tanh', input_dim = len_x))\n",
    "nn.add(Dropout(.17))\n",
    "nn.add(Dense(units = 150 , kernel_initializer = 'normal', activation = 'relu'))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.4))\n",
    "nn.add(Dense(units = 60 , kernel_initializer = 'normal', activation = 'relu'))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.32))\n",
    "nn.add(Dense(units = 25, kernel_initializer = 'normal', activation = 'relu'))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.22))\n",
    "nn.add(Dense(1, kernel_initializer='normal'))\n",
    "nn.compile(loss='mae', optimizer='adam')\n",
    "#classifier.compile(loss='mean_absolute_error', optimizer='rmsprop', metrics=['mae', 'accuracy'])\n",
    "\n",
    "nn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 60, verbose=2)\n",
    "nn.save('keras_model.h5')\n",
    "\n",
    "print(\"x_test.shape:\",x_test.shape)\n",
    "y_pred_ann = nn.predict(x_test)\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "print( \"\\nPreparing results for write :\" )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "##  WRITE RESULTS  ##\n",
    "#####################\n",
    "\n",
    "y_pred = y_pred_ann.flatten()\n",
    "\n",
    "#output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "output = pd.DataFrame({'ParcelId': prop['parcelid'].astype(np.int32),\n",
    "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "\n",
    "print( \"\\nWriting results to disk:\" )\n",
    "output.to_csv('../submissions/keras_nn_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False, float_format='%.4f')\n",
    "\n",
    "print( \"\\nFinished!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
