{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Zillow Algo LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 0.0644038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import datetime as dt\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from zillow_functions import create_newFeatures, data_preprocessing, memory_reduce\n",
    "from sami_function import missing_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\tShape train : (90275, 60)\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "\tOutliers treated ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 44.68 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 78 columns reduced\n",
      "\tFinal size 14.72 MB\n",
      "\n",
      "Building train set ...\n",
      "Preparing arrays ...\n",
      "Training LGBM model...\n",
      "\n",
      "Building test set ...\n",
      "\n",
      "Working batch 100000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 200000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 300000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 400000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 500000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 600000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 700000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 800000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 900000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 1000000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 1100000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 1200000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 1300000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 1400000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 1500000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 1600000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 1700000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 1800000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 1900000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 2000000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 2100000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 2200000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 2300000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 2400000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 108.15 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 2500000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 2600000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 2700000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 2800000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 105.86 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 2900000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 340.46 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 107.00 MB\n",
      "Making predictions and praying for good results...\n",
      "\n",
      "Working batch 3000000\n",
      "\n",
      "Data preprocessing ...\n",
      "\n",
      "Creating new features ...\n",
      "\n",
      "Reducing consumption memory ...\n",
      "\tInitial size 290.13 MB\n",
      "\tThere are 0 columns that cannot be reduced\n",
      "\tThere are 85 columns reduced\n",
      "\tFinal size 90.21 MB\n",
      "Making predictions and praying for good results...\n",
      "Saving predictions...\n",
      "Done!\n",
      "CPU times: user 3h 29min 12s, sys: 4min 20s, total: 3h 33min 32s\n",
      "Wall time: 1h 14min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Version 3 - LB 0.0644042\n",
    "# Train month averages for test predictions seem work better than their linear fit,\n",
    "# so I changed it (overfitting test data as hell... but who doesn't here? ;))\n",
    "\n",
    "## Version 2 - LB 0.0644120\n",
    "# LGBM performs much better, so I left him alone\n",
    "\n",
    "## Version 1 - LB 0.0644711\n",
    "# Both models have the same weight, which is based on cross-validation results, but\n",
    "# XGB model seems to be worse on public LB, 'cause alone gets score 0.0646474,\n",
    "# which is much worse than score of the combination. I reached the limit of submissions,\n",
    "# so I will check how LGBM alone performs tomorrow. Check it out for your own ;)\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "prop = pd.read_csv('../data/properties_2016.csv', low_memory = False)\n",
    "train = pd.read_csv('../data/train_2016_v2.csv')\n",
    "sample = pd.read_csv('../data/sample_submission.csv', low_memory = False)\n",
    "\n",
    "df_train = pd.merge(train, prop, on='parcelid', how='left')\n",
    "print('\\tShape train : {}'.format(df_train.shape))\n",
    "\n",
    "del train; gc.collect()\n",
    "\n",
    "print('\\nData preprocessing ...')\n",
    "df_train = data_preprocessing(df_train)\n",
    "\n",
    "print('\\nCreating new features ...')\n",
    "df_train = create_newFeatures(df_train)\n",
    "\n",
    "print('\\nReducing consumption memory ...')\n",
    "df_train = memory_reduce(df_train)\n",
    "\n",
    "print('\\nBuilding train set ...')\n",
    "            \n",
    "\n",
    "# feature_names = [feature for feature in feature_names if feature != 'transaction_month']\n",
    "\n",
    "month_avgs = df_train.groupby('transaction_month').agg(['mean'])['logerror', 'mean'].values - df_train['logerror'].mean()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "month_model = LinearRegression().fit(np.arange(4, 13, 1).reshape(-1, 1), \n",
    "                                     month_avgs[3:].reshape(-1, 1))\n",
    "                             \n",
    "df_train['super_month'] = month_model.predict(df_train['transaction_month'].values.reshape(-1, 1))\n",
    "\n",
    "feature_names = df_train.columns[2:]\n",
    "\n",
    "print('Preparing arrays ...')\n",
    "X_train = df_train[feature_names].values\n",
    "y_train = df_train.iloc[:, 1].values\n",
    "\n",
    "# month_values = df_train['transaction_month'].values\n",
    "# X_train = np.hstack([X_train, month_model.predict(month_values.reshape(-1, 1))])\n",
    "\n",
    "print('Training LGBM model...')\n",
    "ltrain = lgb.Dataset(X_train, label = y_train)\n",
    "\n",
    "params = {}\n",
    "params['metric'] = 'mae'\n",
    "params['max_depth'] = 100\n",
    "params['num_leaves'] = 32\n",
    "params['feature_fraction'] = .85\n",
    "params['bagging_fraction'] = .95\n",
    "params['bagging_freq'] = 8\n",
    "params['learning_rate'] = 0.0025\n",
    "params['verbosity'] = 10\n",
    "\n",
    "lgb_model = lgb.train(params, ltrain, verbose_eval=0, num_boost_round=2930)\n",
    "                  \n",
    "\n",
    "print('\\nBuilding test set ...')\n",
    "\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "\n",
    "del prop; gc.collect()\n",
    "\n",
    "results = pd.DataFrame()\n",
    "p_test = []\n",
    "batch_size = 100000\n",
    "for batch in range(batch_size, df_test.shape[0]+batch_size, batch_size):\n",
    "    \n",
    "    print('\\nWorking batch {}'.format(batch))\n",
    "    \n",
    "    df_test_batch = df_test[batch-batch_size:batch].copy()\n",
    "    \n",
    "    print('\\nData preprocessing ...')\n",
    "    \n",
    "    df_test_batch['rawcensustractandblock'] = df_test_batch.rawcensustractandblock.fillna(df_test.rawcensustractandblock.mode()[0])\n",
    "    df_test_batch = data_preprocessing(df_test_batch)\n",
    "    df_test_batch = df_test_batch.fillna(1)\n",
    "    \n",
    "    print('\\nCreating new features ...')\n",
    "    \n",
    "    df_test_batch = create_newFeatures(df_test_batch)\n",
    "    #df_test_batch['spe_feature'], nawFeature_mod = create_special_feature(df_test_batch[['transaction_year', 'transaction_month', 'yearbuilt', 'house_age']], model=nawFeature_mod)\n",
    "    \n",
    "    df_test_batch['super_month'] = month_model.predict(df_test_batch['transaction_month'].values.reshape(-1, 1))\n",
    "    \n",
    "    print('\\nReducing consumption memory ...')\n",
    "    \n",
    "    df_test_batch = memory_reduce(df_test_batch)\n",
    "\n",
    "    x_test_batch = df_test_batch[feature_names]\n",
    "    # x_test_batch = sc.transform(x_test_batch)\n",
    "    \n",
    "    del df_test_batch; gc.collect()\n",
    "    \n",
    "    \n",
    "    print('\\nMaking predictions ...')\n",
    "    \n",
    "    # x_test_batch = np.hstack([x_test_batch, np.zeros((x_test_batch.shape[0], 1))])\n",
    "    \n",
    "    # for month in [10, 11, 12]:\n",
    "    #     x_test_batch[:, -1] = month_avgs[month - 1]\n",
    "    #     assert x_test_batch.shape[1] == x_test_batch.shape[1]\n",
    "    y_pred = lgb_model.predict(x_test_batch)\n",
    "        \n",
    "        # del x_test_batch; gc.collect()\n",
    "        \n",
    "    [p_test.append(p) for p in y_pred]\n",
    "\n",
    "        \n",
    "i = 0\n",
    "sub = pd.read_csv('../data/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = p_test[i::6]\n",
    "    i = i + 1\n",
    "    \n",
    "\n",
    "print('\\nSaving predictions...')\n",
    "sub.to_csv('../submissions/light-gbm_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False, float_format='%.4f')\n",
    "print('\\nDone!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
